{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Environment: Apple M1 Max, macOS 15.0, Python 3.10\n",
    "!pip install ipython\n",
    "!pip install matplotlib\n",
    "!pip install mediapipe\n",
    "!pip install opencv-python\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow-metal"
   ],
   "id": "2cf822a85b657545"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-13T11:13:52.234721Z",
     "start_time": "2024-10-13T11:13:52.222507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import resource\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T11:13:54.155768Z",
     "start_time": "2024-10-13T11:13:54.131975Z"
    }
   },
   "cell_type": "code",
   "source": [
    " LANDMARK_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye_inner': 1,\n",
    "    'left_eye': 2,\n",
    "    'left_eye_outer': 3,\n",
    "    'right_eye_inner': 4,\n",
    "    'right_eye': 5,\n",
    "    'right_eye_outer': 6,\n",
    "    'left_ear': 7,\n",
    "    'right_ear': 8,\n",
    "    'mouth_left': 9,\n",
    "    'mouth_right': 10,\n",
    "    'left_shoulder': 11,\n",
    "    'right_shoulder': 12,\n",
    "    'left_elbow': 13,\n",
    "    'right_elbow': 14,\n",
    "    'left_wrist': 15,\n",
    "    'right_wrist': 16,\n",
    "    'left_pinky': 17,\n",
    "    'right_pinky': 18,\n",
    "    'left_index': 19,\n",
    "    'right_index': 20,\n",
    "    'left_thumb': 21,\n",
    "    'right_thumb': 22,\n",
    "    'left_hip': 23,\n",
    "    'right_hip': 24,\n",
    "    'left_knee': 25,\n",
    "    'right_knee': 26,\n",
    "    'left_ankle': 27,\n",
    "    'right_ankle': 28,\n",
    "    'left_heel': 29,\n",
    "    'right_heel': 30,\n",
    "    'left_foot_index': 31,\n",
    "    'right_foot_index': 32\n",
    "}\n",
    "\n",
    "COCO_KEYPOINTS = [\n",
    "    'nose',\n",
    "    'left_eye',\n",
    "    'right_eye',\n",
    "    'left_ear',\n",
    "    'right_ear',\n",
    "    'left_shoulder',\n",
    "    'right_shoulder',\n",
    "    'left_elbow',\n",
    "    'right_elbow',\n",
    "    'left_wrist',\n",
    "    'right_wrist',\n",
    "    'left_hip',\n",
    "    'right_hip',\n",
    "    'left_knee',\n",
    "    'right_knee',\n",
    "    'left_ankle',\n",
    "    'right_ankle'\n",
    "]\n",
    "\n",
    "# Maps bones to a matplotlib color name\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "color_map = {\n",
    "    'c': (0, 191, 191),\n",
    "    'm': (191, 0, 191),\n",
    "    'y': (191, 191, 0)\n",
    "}\n",
    "\n",
    "KEYPOINT_DICT = {body_part: LANDMARK_DICT[body_part] for body_part in COCO_KEYPOINTS}\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {tuple(KEYPOINT_DICT[COCO_KEYPOINTS[i]] for i in k): color_map[v]\n",
    "                               for k, v in KEYPOINT_EDGE_INDS_TO_COLOR.items()}"
   ],
   "id": "73384d4acd90b274",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T11:13:55.122615Z",
     "start_time": "2024-10-13T11:13:55.114349Z"
    }
   },
   "cell_type": "code",
   "source": "KEYPOINT_EDGE_INDS_TO_COLOR",
   "id": "5f9eb031c41d9863",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 2): (191, 0, 191),\n",
       " (0, 5): (0, 191, 191),\n",
       " (2, 7): (191, 0, 191),\n",
       " (5, 8): (0, 191, 191),\n",
       " (0, 11): (191, 0, 191),\n",
       " (0, 12): (0, 191, 191),\n",
       " (11, 13): (191, 0, 191),\n",
       " (13, 15): (191, 0, 191),\n",
       " (12, 14): (0, 191, 191),\n",
       " (14, 16): (0, 191, 191),\n",
       " (11, 12): (191, 191, 0),\n",
       " (11, 23): (191, 0, 191),\n",
       " (12, 24): (0, 191, 191),\n",
       " (23, 24): (191, 191, 0),\n",
       " (23, 25): (191, 0, 191),\n",
       " (25, 27): (191, 0, 191),\n",
       " (24, 26): (0, 191, 191),\n",
       " (26, 28): (0, 191, 191)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T11:13:56.263786Z",
     "start_time": "2024-10-13T11:13:56.255329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_image_with_prediction(idx,\n",
    "                               raw_image,\n",
    "                               keypoints_with_scores,\n",
    "                               keypoint_threshold=0.11):\n",
    "    raw_image = np.array(raw_image)\n",
    "    keypoints_with_scores = keypoints_with_scores.copy()\n",
    "\n",
    "    raw_height, raw_width, _ = raw_image.shape\n",
    "    longest_side = max(raw_height, raw_width)\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert relative coordinates to actual coordinates\n",
    "    keypoints_with_scores[..., :2] *= longest_side\n",
    "\n",
    "    # Offset the coordinates based on the aspect ratio\n",
    "    if raw_height > raw_width:\n",
    "        keypoints_with_scores[..., 1] -= (longest_side - raw_width) // 2\n",
    "    elif raw_height < raw_width:\n",
    "        keypoints_with_scores[..., 0] -= (longest_side - raw_height) // 2\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve values from the output\n",
    "    kpts_x = keypoints_with_scores[0, 0, :, 1].astype(int)\n",
    "    kpts_y = keypoints_with_scores[0, 0, :, 0].astype(int)\n",
    "    kpts_scores = keypoints_with_scores[0, 0, :, 2]\n",
    "\n",
    "    # Pair up keypoints to form edges\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "        if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "                kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "            x_start = kpts_x[edge_pair[0]]\n",
    "            y_start = kpts_y[edge_pair[0]]\n",
    "            x_end = kpts_x[edge_pair[1]]\n",
    "            y_end = kpts_y[edge_pair[1]]\n",
    "\n",
    "            cv2.line(raw_image, [x_start, y_start], [x_end, y_end], color, thickness=max(longest_side // 300, 1))\n",
    "\n",
    "    # Plot the keypoints\n",
    "    for i, coord in enumerate(zip(kpts_x, kpts_y)):\n",
    "        if i in KEYPOINT_DICT.values() and kpts_scores[i] > keypoint_threshold:\n",
    "            cv2.circle(raw_image, coord, radius=max(longest_side // 150, 2), color=(255, 20, 147), thickness=-1)\n",
    "\n",
    "    # Convert RGB to BGR\n",
    "    bgr_image = cv2.cvtColor(raw_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Save the image\n",
    "    cv2.imwrite(f\"./output/{idx:08d}.png\", bgr_image)\n",
    "\n",
    "    return f\"./output/{idx:08d}.png\""
   ],
   "id": "f3bc14d8ca381920",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T11:14:34.867251Z",
     "start_time": "2024-10-13T11:13:57.226373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_root_dir = \"./dataset\"\n",
    "raw_images: list[np.ndarray] = []  # Each entry is a 3-channel RGB images [0, 255]\n",
    "for dirpath, dirnames, filenames in os.walk(dataset_root_dir):\n",
    "    dirnames.sort()\n",
    "    filenames.sort()\n",
    "\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(dirpath, filename)\n",
    "        file_extension = os.path.splitext(filepath)[1].lower()\n",
    "\n",
    "        image = tf.io.read_file(filepath)\n",
    "        if file_extension in ('.jpg', '.jpeg'):\n",
    "            image = tf.image.decode_jpeg(image)\n",
    "        elif file_extension == '.png':\n",
    "            image = tf.image.decode_png(image)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Ensure image is 3-channel\n",
    "        image = image[..., :3]\n",
    "        raw_images.append(image.numpy())"
   ],
   "id": "3873e965595197cc",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T11:14:34.877186Z",
     "start_time": "2024-10-13T11:14:34.872133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_inference(model, image: np.ndarray) -> np.ndarray:\n",
    "    # Perform pose estimation on the image\n",
    "    results = model.process(image)\n",
    "\n",
    "    keypoints_with_scores = np.zeros((1, 1, 33, 3))\n",
    "\n",
    "    # Check if any landmarks were detected\n",
    "    if results.pose_landmarks:\n",
    "        for i, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "            # Each landmark has x, y, z coordinates (normalized)\n",
    "            x = landmark.x * image.shape[1]  # Scale x to image width\n",
    "            y = landmark.y * image.shape[0]  # Scale y to image height\n",
    "            z = landmark.z  # z is already in relative depth\n",
    "            confidence = landmark.visibility\n",
    "\n",
    "            keypoints_with_scores[0, 0, i] = y, x, confidence\n",
    "\n",
    "    return keypoints_with_scores"
   ],
   "id": "daadfef7ba5bb694",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T11:14:35.865111Z",
     "start_time": "2024-10-13T11:14:34.937110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "with mp_pose.Pose(static_image_mode=True,\n",
    "                  min_detection_confidence=0.5,\n",
    "                  model_complexity=1) as pose:\n",
    "    start_time = time.time()\n",
    "    results = [run_inference(pose, raw_image) for raw_image in raw_images]\n",
    "    end_time = time.time()\n",
    "\n",
    "print(\"Total time spent:\", end_time - start_time)"
   ],
   "id": "49e1b8d8eb9d76c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1728818075.281078   93445 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1728818075.438000   93445 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 17 is out of bounds for axis 2 with size 17",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m mp_pose\u001B[38;5;241m.\u001B[39mPose(static_image_mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m      3\u001B[0m                   min_detection_confidence\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m,\n\u001B[1;32m      4\u001B[0m                   model_complexity\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m pose:\n\u001B[1;32m      5\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m----> 6\u001B[0m     results \u001B[38;5;241m=\u001B[39m [run_inference(pose, raw_image) \u001B[38;5;28;01mfor\u001B[39;00m raw_image \u001B[38;5;129;01min\u001B[39;00m raw_images]\n\u001B[1;32m      7\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal time spent:\u001B[39m\u001B[38;5;124m\"\u001B[39m, end_time \u001B[38;5;241m-\u001B[39m start_time)\n",
      "Cell \u001B[0;32mIn[38], line 16\u001B[0m, in \u001B[0;36mrun_inference\u001B[0;34m(model, image)\u001B[0m\n\u001B[1;32m     13\u001B[0m         z \u001B[38;5;241m=\u001B[39m landmark\u001B[38;5;241m.\u001B[39mz  \u001B[38;5;66;03m# z is already in relative depth\u001B[39;00m\n\u001B[1;32m     14\u001B[0m         confidence \u001B[38;5;241m=\u001B[39m landmark\u001B[38;5;241m.\u001B[39mvisibility\n\u001B[0;32m---> 16\u001B[0m         keypoints_with_scores[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, i] \u001B[38;5;241m=\u001B[39m y, x, confidence\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m keypoints_with_scores\n",
      "\u001B[0;31mIndexError\u001B[0m: index 17 is out of bounds for axis 2 with size 17"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "memory_usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss  # bytes\n",
    "print(f\"Memory usage: {memory_usage / 1024 ** 3:.2f} GB\")"
   ],
   "id": "27267e68b429429e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def show_and_save(image_idx):\n",
    "    output_path = save_image_with_prediction(image_idx,\n",
    "                                             raw_images[image_idx],\n",
    "                                             results[image_idx],\n",
    "                                             keypoint_threshold=0)\n",
    "\n",
    "    image = Image.open(output_path)\n",
    "\n",
    "    # Display the resultant image using Matplotlib\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide the axis\n",
    "    # plt.show()"
   ],
   "id": "b0cbd928b5a8bccb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "for i in range(len(raw_images)):\n",
    "    show_and_save(i)\n",
    "plt.close('all')"
   ],
   "id": "62d78016f99fa9d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "677a88c233de9d97"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
